make sure like this is the floor understanding will build in a small modification yeah we'll go for no shifts So firstly I'm checking the by pinging each vm based on his availability if the pm is available I will wait for 3 seconds if not if there is no reply i'll consider the pm is not that vm is not available the first vm for which the ping is successful will be selected and that other ip will be fed then this logic has changed as per discussion I will use delete a file instead of deleted table so early what I was doing here I have created my table which carries the autosys status keeping the name of file job id status and this autosys yes yes so it will be creating catalog only yes but I'll be changing the data file since I understood you guys need for park it in the background it creates a delta only yes because in the background it is a delta no doubt about it if you go and see that one you are creating the table but we need to create a table if not exist yes exactly what you did amazing I think that Delta table also good option I did not think about that on that time see in the bigger background it is going to create the tables only I think that you are old approach i'm going with that I feel that this also good option only ok yes I want you to create in the particular in a new tab can you go to this compute catalog can you go to the catalog you create under the E to use to be up if you are able to perform perform under the default this one can you create here ok owner is good and this should be dynamic because in the higher order we can put a data also no issues ok let's put out so that you will not have any issue so under the df created everything ok this one ok I will default you see any challenges you are facing on that one but this is our dedicated unity care club which is centrally managed so I would recommend to create here rather than creating on the default default it is specific to work space we may not able to access any we are losing the space we are out of Delta Table I will do that I will create it in this one see that one probably you may have a need to see any access issue or something can you run the query now let us see under that one let's see whether it is going well so either way is fine lets go with the approach I mean let's let's go top the approach ok so once we have this auto sys table being created what I am doing is I am again I am I am trying to log this status based on the job id file name and what is the current status it will install this particular in this format the information about a particular job id column go to the columns so this always should have the data data as update that is all column this timestamp is the actual walk up what is the date if you keep the time some no issues which you will have milliseconds but I wanted to have a data as update see any file processing when we do through our data branch we will having that there is a field called data as update that is a partition data we will be using for example assume the data is growing so we need to see the option of partition as well but we need to have the data assembly information so I'll add that in as well so whatever the job id will be like job is kind of whenever we are running a job right there will be a particular unique number assigned to a job that is a job id it can be alphanumeric it can be entire numeric but right now I kept this string only to make sure that if it is alpha numeric it should not be failing right so I am going to store or just to have the uniqueness about every so if there is a same job ad is coming back we can assert it right I need to see I haven't fetched that thing right now it is just formatted right now so if ok we wanted to make sure that maintaining the uniqueness on the file so same file process consider that we are having the process of the same file name twice means how are you going to handle the upset under status we need to see that as well always update the latest status ok let's do that way let's keep filing as a web condition and now we update this status plan which we have 2 entries for that file name father same name data as update ok we need to control in that way just for your information our processing is based on the everyday like it starts at 3:00 mm-hmm and so next day 3:00 OK OK ok so once I have the log information like file name i'll remove this whatever changes you said for the job id and include the data all these logging information we will store once we will store the execution time stem of everyone in each as a new record entry if the record is already there but we will have this execution by default three things we will do default huh I will I will remove that one S that we can have the absurd logic because time stamp will be creating a duplicate not duplicate but the new entry so then once we have this information stored here the log log status is available the next next plan is to create a notification table which we discussed right I am using the same format for the notification to have the notification information so once we have the logging available I will show you one dem workflow so for example I got a vm being selected here now I have this job id I have the job id and job status available I stored the log information then I create a notification regarding the job status OK so once we have the job status available which is logged in auto sys table we are going to update this yes so for that for that only the stable I have created so initially by default the job status flag will be false for this one once the it is it is successfully completed we will mark the notification as notified and will update this status so the flow will be like we select the vm first then we have the job status we update the job status in our table we will check for the notification we will share the notification and mark the notification notified this is the sequence ok can you repeat the after the job status I understand default so status is not yes so before autosys flag is false once the job is being successful a job is finished we will update the notification notification will be like first we will in notification table we will update the status as successful this is successful then we have a notification table which carries the information regard for the Kafka point of view whether the notification is being released or not once the jobs completion is successful we release a notification and will mark it as a notified so is it understand this one you are not going to handle any success failure will happen no irrespective of what you are doing autosis is a completely independent part of Kafka ok during the Kafka notification either success notification or such really notification we are making the lock stair job status update we are doing ok with the entry of success or failure with the default value of autosys no ok that's all you have to do on the notification one OK mm-hmm I think that I think you are making a very good progress OK i'll I'll I'll actually to be honest I like your approach the way you are doing the POCI love it I actually everyone has to follow like this for us to do the small working solution and then integrate over there I like it ok but just to correct your understanding on that one we are not controlling the notification we are making this log update on the while making the notification we have A we are in the end of our process OK just we are updating the status then later every 15 minutes when our job runs we will be using I don't think you need to work on the job job notification part I think it is extra work I don't need no work OKOK Sir I did that because just to make sure that solution is working so yeah that till now this part is done so nothing else later now I will just I was about to work on this table chain instead of file but now you said I don't so I won't default see I want to see do you face any access issue then we can go to Delta C if the concept is same whatever you're doing is a good job instead of Delta table you are going to delta create that's not a dial issue I think but you need to create in the right unity catalog that's what I meant but you have creating the unity catalog i'd be happy only I think it makes sense I think your approach really good thank you sir actually the main main thing helped me was this particular thing earlier I was not able to get the IPC1 so once I saw this loads balancer api this has everything including ip now what are we going to do here we are going to integrate your code changes and then we will perform in the test where the two were machines are there it has to do the your code as to work properly that is fine now you bring it down I want to work on the another part ok so every 15 minutes we are calling right I think you done the job update and then we need to autosys you found the job fine and then instead of success we need to read the failed I mean are you perform that state also we need to read the Delta which is having a status of no only and then we need to form few things probably will integrating changes we can do please start working with Mauli you create the future branch and set up a call probably today afternoon I will tell you where and tell you start working on the integration part and perform the testing on this one ok sir so let me be clear and what I need to do is 1st thing use in Dub 2nd thing implement after is update this logic with 15 minutes check right now work on 15 minute logic where we will fresh the status in 15 minutes not before not after right know that that logic you don't need to worry ok stop sharing ok to make you understand yeah that's what I was thinking this this little grooming set but you you are already in a good shape I can sense it it's good ok thank you sir I was wondering if this little grooming session where I can otherwise this notification thing I wouldn't have done but my bad I feel that way what do you call you call I am not going to tell you So based on that we'll be reading every 15 minutes this one how 15 minutes works is from the ADF we have a schedule to run this pipeline ok this pipeline what it does is it reads ALDLS notification and then it identifies some comments I will tell you you you need to go through this pipeline also once do you try to understand before the call you are going to have with me in the afternoon okay okay it identifies what is a success comment and what is a failed cover based on the success and failure there is a certain specific format of we need to form the commands to run and the autos is client don't worry about that one all the logic is here but we are going to change instead of this logic we are going to use your new notebook where it will perform everything until here what we are performing is we will be forming the which will be executing on the auto sys comments if you see here we will be calling the this vm this pipeline we have exude command so for this we have exude command every 15 minutes we form the subset in command the job status as a success or failure but until this part it is going to replace with your code whatever you are building before this is condition ok I am calling on notebook only OK if there is a any changes are there to update so I will be calling the notebook OK so all this any question feel free to ask so I do this file if there is a 35 is there all will be removed and only your logic going to work so why I wrote this logic is I am reading on adls finding last 15 minutes it rise for 50 every 15 minutes right so I will be checking every 15 minutes the adl is having any success message what is success what is failure message and then I am trying to execute on the autosis client which is nothing but the vm which you are trying to perform there think test so all will be eliminated and your new notebook will be called where you can able to give the success message and failure message to us and then based on that one we can go and update the other 6 comments okay your notebook will be there we are going to just identify what is success and failure I mean the last stage I asked right after setting that work you are going to make a delta query with a select statement with the status is no you are going to identify what is access failure we are going to form the one statement and then we are going to solve this program so any questions here only one question yesterday you said there is 1 particular function being created which we can use to call use a per current to run a job I will try to see in the details but not able to find maybe I was overlooking it so that one how are you going to integrate I can definitely guide you since it is a first project I really I don't want to put you in the bin I want you to work and we promised we are going to release the changes on March 8th okay so now you need to start working on the integration part so please work with the Mauli and understanding the gift process slightly and then we will in the afternoon do you before that you try to understand this flow also we will discuss ok sure sure so I will do that yep I will connect with Molly and before that I will make sure that I have some highlight understanding so if any questions i'll reach out to her and then we'll come to you ok Alright lets go to any other open target everything to discuss if you like you can bring it up now yep information yeah actually I have doubt on like I want some understanding from Maundy on this fifty she has written right so just function right what is your question actually I will even call you can help you so regarding that particular what you did and why you did that so as I understand that what is your understanding what is your question that's what we can answer see she might be given the what functionality we can't explain
