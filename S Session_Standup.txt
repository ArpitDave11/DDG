Exactly completeness check that was what I said if you go to that yeah this this process it's kind of what would be the previous one previous diagram are you able to see that mainframe injection yeah but it's blurred out somewhat yeah yeah I know what's the problem give me a minute give me a minute give me a minute my screen is it clear now this one right this point is in black now but maybe you need to size up a bit how about now is it clear to everyone bit more yeah I can see dear we we we we do right now right so now this exact thing when it was published we need to do for underneath and I believe the ontology file i'm sorry II have some noise so I don't know i'm giving one give me one second so guys you understand right what's it this is not time to emphasize here when we are creating ontology we are becoming a producer so we need to kind of you know follow what is being you know done by the our producers right like our mainframe in a producers what they are doing it the same thing we need to do it here for for when we are kind of in a producing a file for the tone stream consumption so it has to follow the entire you know the 9 steps what we are having it so all the source files which is we are getting it today for this building the segment views which is going through this nine steps so when we are producing a new file segment then we are beginning producer we need to do the same thing that is the crux of the message what is trying to kind of say on on the mappings which are in available for foundation domains what we have account party positions transactions settlement and everything but we are not producing a solution ontology exists right I mean we're using wherever we just team produce in the anthology yeah so we go with Visa's team is visa team is nothing but visa team is defining the ontology structure yeah yeah yeah we are doing only requirements right mike so they are generally requirements source to target mapping and the schema they are giving it but we are the only one really implementing it and producing that file right yeah yeah the file but not the ontology so ontology is defined we just producing the file according to the source of target mapping in the college I don't say I'm like strata modular profile provides a data model and the data engineers develop that so ontology is kind of a data mode we can say and correct everything that we follow right now has to be implemented the same structure right the same we can't do the same thing probably like the checksum match or something those we can't do but yeah some of the things like the notification publishing account you are producing you cannot do a checksum you produce a jigsaw into the notification message and the notification message need to be properly formatted so that if anyone wants to do some validation or something that they can use Yeah but also archival exception all those processes we should follow and here here my question is that here we are producing that means we are creating a control control pilot manifest by where my kick some and the record count that needs to be published and 1851 segment file that we are producing right and the second second very important thing is that each segment file become a registered registered file or registered entity it is not it is a data that has been produced so if the data is not part of the registry then the data doesn't exist in in in English we cannot bypass anything with that because that that is all it's like the way we are together making entry for the main frame files it's like we need to make an entry for this segment use also correct segment use also because that segment is if there is a new structure needs to be needs to be introduced into our register file that's fine it's a different structure I can have different different object models there right but that the segment files we can have account at our under account I have a 10 fines that can be 1 segment right so that when you are at with one block in our history file when you have another segment added into under account that should be registered because why it is very important because we are now so now what will happen is just wonderful once you done with this engineering now that will go to the first disproportionate is produced right but satisfy that term is similarly on but it is fine based on what like the difference in sector right and when calls are in 5 we want segment of the data flow as 100 when you create a second time expected on the end of the data should match right so that's how our whole database will be provisioned now with that the next step is the database will be whatever the ontology files and everything that has two next step is to go to produce the entitlements that is on the journey registration available so these data that you are producing for the more of these buyers I understood distribution of these data codes OK that's fine paper example for real time or a real time data that are exposed or thought that are pumping at least or ontology account there will be one output port which is a denoto view which is a denoto VDB port right similarly for the for this segment views or end of the day we have two output ports 1 is the fine itself that we produce and another is the file and another is the denoto view so that's how the military store where every everything will be captured what we will whether we will register in China or not that we will decide but this schemas and the columns needs to be published then then next step OK the meter data stone capture so now currently there's better at the store loading is and modeling and automate that there are slaves but down the line this whole thing is going to be going to be exposed or going to be published into the sales and portal so so if the sensor portal what we accommodated to mark is will be a internal portal where the data owner can see my data sets and the definition of the data sets and they gonna do a soft approval there whether that data set to be taken and population so in the center portal if you see there is ADRS registration button that is going to be that needs to be at that screen or that needs to be little bit rebound so that attach part we need to take care of and find out how we can capture that evidence that that data owners say OK yeah we are good with this data product please publish it due to Janus in that next step of automations will kick out where uh where this data already in metadata store in the wrong there a automation when invoked the DPS framework to register the data sets into library seven data sets and then in the product into genres mike and Mike and Adam can open more half the details of it and I will let's discuss that registration process in the next session someone and how it's gonna work right and and I'm going to demo now if you're currently working right now in each segment so it's one placed into the channel as a data product so now this now once the data products are published the actual approval the UPSY the approval will be happening on the Janus so now Janice will talk please will once the data when we publish into Janus it will be in a draft state and the data owner whoever is will be doing the the approval through there and it will be the one cities approved the data set is actually available for UPS to consume so in if you if we go in the next session might demonstrate that how tennis people looks like how the data will be accesses accessible from the output ports so output ports in the Janus portal you will see that the connection strings that connections strings will need to directly to the file or the data sets that you are you are publishing or you are exposing into the genus so it we cannot mark anything here it has to be really real thing has to be stitched together which leads to the final final publishing match by publishing product into the Janus from where the data sets would be can be accessible now here is the big thing comes next step is that how we establish the entertainment so entertainments are that as a user or as a technical account who is at who is who has permissions or who should be able to access the data that's what the general guidance is how UPS defining both in the CID data and how the data classifications are done so if you go to anybody can go to go to CID you will see there are there are 4 types of CIT data and categorize ABCD and and there is a non CID data that's how the five category exists the cat a and the CIT data cat ABCD are defined into two categories as a as a rack state right rag red amber green sweat ACIDA 2C is categorized as rate CIDP is categorized that the number in C8 and non CIT columns are categorized as green so implement this whole thing in the noto because it is that's how the Dinoto entitlements will work so each account say account as A database and it has certain data sets which are you can you can think about cables that are coming each tabs has columns so the tables will be unloaded with a tag that is read as a first step for all columns that will be unloaded as a right so basically what what does that mean that when you go to PBS and you act and I want to access the account product as a user or the technical account you will you will require to choose what type of access that you need whether you need clean amber or rate that's how you gonna be accessing the data and that to implement the data that is a another big step that needs to be done is that Each table has a couple of columns each columns needs to be classified as red green amber above the classification of for the next state and and the classification of the column column classification of CID cad A we get CD so that's how the tagging process will kicking into denoto where this will be tagged of each segment and say each segment if I go to say account address account address at the universal account number and the sequence and in the segment and you have a address address in that address block that if any of the columns because it's A overall Json Json data so that is should be cheated as a as a singular column because at this it store that value right in the value there are multiple columns and all this if any of the columns has rate then it becomes the whole data become red so that's the that that we will assume things are assumptions and that assumptions will be will be published as a documentation that how you know or when when you require what type of roles how it will be translated into the actual data or when you see the data so if you if account address segment is classified as or account address segment that address release classified as CID so then you need to require to have Android and that translate that my account account address accounted at account whole database having a rate categorization also by the table because it has and that we can discuss whether we should classify the whole table as a rate or we should classify that or we cannot classify that table so that we can discuss but this is how it will be sticking together so the entitlements that are getting now built is based on each database and each database in Denver and then Dinoto has the main virtualization tool for now and the users will raise corresponding corresponding pbs roles so they have pause here anything might add on anything I have missed here no but who's presenting right now online presenting I am the only why we having account on the account why are we having account on the data fabric I mean it looks like it's all over the place so it is kind of just modeling of mic the account under ontology under data OK so now that the meeting signals up the main step you presented suburban segments for that specific domain and the entitlements will be on the based on the main right so so and the tagging will be also based on the domains so it will be entitlement will be called account I don't have speed I don't ask we cannot like translate this whole model which will lead together to the general site no no yeah I understand we just do it right again I mean it looks like it's working progress so it was a little bit different the last time we spoke so OK and then wherever Nick was saying about the base views and then derives you so that's that's I mean is that a discussion still on or in a title and go everything is gonna be on the on the base views and on the segment so how is it going to be represented ohh that's right you're right no I understand what they did what is up or there are ideas so that that's where the technique nick will go on but the bottom line is if there is a sub domain if it is coming directly the data is coming from like a data engineering pipeline yeah it's I think it might be still surprise that I don't need to create any derived view how big is OKOK Alright OK and so with the same with the concept OK so if data if if data represents the file then we stay with a base view and if if it's multiple files then we created the right view OK yeah multiple cables from where the data or the view is constructed then we'll use that ti view right but we can for the sake of the right view based view that's what we don't need to do right that's fine I just want to have this conversation again by snake because I mean again I just agreeing go on because we carry this completed every few months we're having the same conversation over and over again it goes on right i'm I'm not discussing anything with this one approach with me anymore we'll be very honest sorry because this is what if and if if there is any anything that we need to clarify we will we can go to Dinodo and and review our architecture and see their recommendations right we don't need to go over it again and again because it's entirely depends on the data engineering that we are doing how the data is getting published or there can be based practices but based practices are are for adoption not for reinforcement right so that's the view of it so now that is how the whole thing will stitch together this contour of the transformation how clarifying question before I give me one second before I go target the another part of it that the same serve right the same serve we we we need to build two things here 1 is the schema definitions or the data sets that we are we are going to do is the metadata store it should be having AA ui for it a screen for it basically because no matter in data engineering no matter what we do if until people cannot visualize these things nobody knows what we do right so the ui will be our our like way of representing what is delivered and what is not done right I understand so yeah we also need to kind of bring this account segment views also into our master authority into the self service portal right just like the way we are doing the mainframe files OK so that is searchable for searchable for the for it now now with this approach right now now with this whole thing in data engineering that we're doing the three basic use cases comes out as A immediate AI use case is that for the cables that I got or for the all the data sets that I got I need three things to be done 1st right in order to do that and then 3 things is my table 8 data sets or tables views whatever it is right which is treated as a data set that needs to have a description second is each data set has columns that columns has to have a description each columns and third is each column has to have a classification whether it is the data is rate green Amber or something or sorry the rack status and the 4th is what category it belongs to the data belongs to so the rack classification use so that's where the our 3 AI use cases comes into picture that we have certain certain informations about that at the table descriptions and we get the data there is certain information already exist how we can teach these these existing informations and in reach our metadata store so all this and all these data that will be interesting to our meter data store where each schema will have its definitions all these columns that are blank now that is to be populated that is the output of our three use cases that we we are picking up right once you are done sorry that will enable this the file agent what we have right that will whenever we do the prompt or anything it will you know you will vectorize the data correctly on the file yeah lot of other use cases because now you need to correlate and create another domain or another product these these whole basic things will enable those those ai enablement will happen after down the line right that's what may be that's my understanding or you know you are right yeah what is the 3 the 3 are the descriptions the CID level what was the 3rd use case the color description column and table right a table and column no classification under the data classification there are two parts of it 1 is the rag which is green amber and identify and the second 1 is identifying the data classification whether it is a cid ABC that's what we need to identify so we each have two descriptions table columns and then see it in like classification CID but also it's like 4 2 and 2 but you you want to do those I mean this is kind of II know this the product is like some path was saying we need to do the 4 first and then the file explorer everything else propagates after when we are getting forward 11 these fours are done then we have a solid metadata which can enable AI for other use cases right so this is this is whole purpose of these four use cases to enrich our meter data in the store right tomorrow tomorrow like 2001 22 broad categories one is description generator description generation for table and column and second is classification generation classification generation is rag and cit classification both are same like cid classification and drag abcd and are red and redirected so if you if you open go to cid then that will be yeah yeah yeah go down so one is the cat a cat B Cat C Cat B and non CID that is the five classification that we have to come and second is the rag what is red amber green so so either way if we do rate a rag first cid can do next that's why we said drag is required 1st as a priority because rag directly entitled leads to the entitlements so rags to do 1st and then is a next approach as a next 1 we will we will tackle that Cathay cad BC nonsensitive we would need to probably do descriptions before anything because CID would be hard to do if we didn't know what it no no no don't CID is hard to do is different but for for identifying whether it is ACID if it is a cid then I will we will not use as amber we will use classify as rate is the difference you would call them that would be the better way to phrase it you gotta so that is the 1st part these two is parallel that we have to tackle because that required for entertainment so if I say priority wise I need rag first which is parallel to descriptions what is the plan time I mean the rag is a just embedding right so what is our exact plan what data we are going to put into that this is rag is in red not ragged I was thinking I was a little confused sorry probably I was offering videos I've done this before it's like AIAI confused with alternative investment and AI right so you are saying something sorry can you pitching no I was just saying like anything on this is started so that's what the grouping that's what the grooming you already set up a meeting that's what the grooming would do so this is the ai use cases that is that is first thing right and that and the a pixel already shared right I guess yeah yes started Harsha Hashad I think started so far we are just kind of having the discussion work I mean Adam we did start that's I mean I guess that's I mean that's my confusion let me finish because I'm like we started and I'm developing a plan with you know you mesh and RP and now we have Shivam so I would like clarification on like what should I be working on because II don't want to make like I'm right you know I'm spending time on making the plan for description generation so we can olive and like yeah is the reason for this division yeah and so all we have to do first is that the room the epic you you are developing the at the plan that is that is that is the correct way of doing it like just let's let's parkroom the epic properly let's find out what task how we gonna be approaching it documented then then you continue the work right we have a guided work that needs to be done so that we see the end of the day because if you see a particular different opinion I will have a different opinion you will have a different evolving place right I think I mean in terms of A It's not a straightforward recruitment which we are working for any in terms of this engineering work I feel that we should have a collective discussion on the what is the approach we are coming I feel we are saying the same thing right that's what I'm saying you we should not be doing in a silo or developing something that's what even if you are develop something let's discuss it that's what we should be having that this is what we are doing right because it's nobody is asking like yeah I think the description and CID could start parallel and the idea maybe the description will also be utilized in the CID category because the CID category however we developed it will lead some information right not only the data but the column description so it might need that exactly but at this point and that's where a common discussions collective discussions are required right what we are doing now because end of the day how the date whatever you do right end of the day it will drive yesterday is a normal nature that schemas will be surfaced into the the safe service portal where the DMOS can come and opine and correct and correct the description and commit that one S that that gets into the feedback loop of my the meter data store getting enriched correctly I see it so we have discussion with vision and we create on different she's maintaining ontology she will go out to respective business owners of the KDNCIDS and she will bring that information to ontology we need to create the sync service that will bring it from ontology she's the golden source for the CIDCIDID is for KD's that is what we need to do that's we need to tag right all I'm saying in the middle store that information to be captured right yeah when when we that agree right so our end delivery whatever is the metadata store that should be in my in our self service portal where that where the DMO can review it right what that's what I mean our all the data owner can see it whether that is correctly tagged or matching to the whatever they have published and they can they can approve that OK these data sets are good publish it into Janus that's what marks requirement all the all that data sets that are getting not yet ready for Jenna's or everything so they want a real name that you just said the only thing is that when they reviewing the data set and reviewing the categories they will not be able to change them OK if they disagree with something they would have to go and change the dermatology because you maintain your ontology but the data owner right so ontology data owner would be whatever this but there are legacy data source CRTBCRTV owner sums and say OK this is by CRTV data sets which I need to change to CRTV owner will be able to change it because I have I have CRDB databases tables right the my legacy database legacy data stores or data sets are coming from CRTB or PRTV if that PRDB who are we to block that they are the data owners psi visa if I say visa is owner of the ontology definition similarly the application data sets that are coming that will be the different 1 that different owners that so guys we are reaching the top of the oven so let's kind of you know if you guys have any questions on this particular topic on the ideology piece for so for this meeting for tomorrow then instead of Harsha II can attend cuz it's during like our commuting time but what what is like the goal I I'm just confused over what is the goal for epic grooming or like what what the process is for the use case because I yeah I can present like here's the research I did and here's the RP and you mentioned in the last couple weeks and then are we just gonna discuss on who's doing what I think for the next no no when we move past to Friday then it's moves that harshad do you have any problem to move that in Friday yeah let's do that Friday I am at 9:00 AM set up on Friday already for this like but for our you know for Umesh Shaban our key deny I had this set up for Friday at 9 right now but so we can include whoever can we utilize on Friday what Adams you can you can forward it to us adam may be just what you are looking for you can forward to me and Jyothi I will put the people please yeah me also adam so that they now make sure that it is not application ok that's what you are having right no and some of the there is a 1 meeting just keep on rescheduling related to the data machine or what is that even I also don't know when I need to check here it's a very valid one you can check it a lot of newcomers with a background right hmm what exactly is that is yeah so Adams Adam also involved also for include Mike here also and Pinaki because all of them are required right right absolutely rather than only focus on the product description we need to start because she will miss there and a lot of new people right I think we need to start looking for the other use cases are also actually in addition to everyone is okay **** on the same use case i'm thinking about your view I think let's III agree but let's start with this one first okay other users will go and see if the abuse is a great right if you have a uses we can have that done right but the approach is that we have to take it somewhere to the production OK Alright and and so are real value of right right we have browsers like browser ai some people can analysis is the is the 2nd one that Ghana has done some part right we have to take it to the production and actually and actually implement into our overall like day-to-day engineering whether it actually evolves or like solves our problem similarly the intelligent qa automation is where is another use cases will come out when we discuss the approach then the use cases will come out more where is ideology this end mapping to the database notebook right that is another one which can also be seen here so the focus will just be on description which I guess is also right until we get our product 'cause we you're saying we could make multiple POC's like if we make one with open AI 1 with Jeannie like we'll see what works best to be done right and then similarly the other areas that use cases will come out and once the use cases are beat by your whether it will actually work or not then we will pull we will put it here we can have POCS right OK automation harsa has some use case on board that use case right what Kali is saying we have to all look for other use cases that's perfect but the bottom line what what we should be doing is that my recommendation is that that should be something translatable to a particular delivery it cannot be just ended with an POC and done we get it great thing forward to the production anymore right so when I come up with this case really sowing at the ai you're doing about the same right how very implementing those things for data mixed up the other description I understood and that is not a difference here so if you can I will tell you what this case is now in few minutes right for this if you go if you have some questions related to implementation and the registration and how it kind of Indiana flows into Janice you know classification access and everything feel free to ask the question because without understanding the full picture it is very difficult to kind of you know implement properly so you know please post your questions and that will help you know to kind of see if there is any gaps anything which we need to kind of you know address it also and as I said
